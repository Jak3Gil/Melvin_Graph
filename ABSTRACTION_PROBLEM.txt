# THE ABSTRACTION PROBLEM

Similarity works for surface features:
  cat ↔ bat (share "at")
  
But NOT for abstract patterns:
  1+1=2 vs 2+2=4
  
These have LOW similarity (different numbers)
But HIGH structural similarity (same pattern!)

## How Does Abstraction Emerge?

### OBSERVATION:
When you learn "1+1=2", you don't just memorize it.
You extract the STRUCTURE: "X+X = 2X"

### THE QUESTION:
How does the system discover that structure from data?

## Possible Mechanisms:

### 1. HIERARCHICAL NODES
```
Level 0: "1", "+", "2"  (tokens)
Level 1: "1+1", "2+2"   (expressions)
Level 2: "X+X"          (pattern - abstract!)

Create level 2 when level 1 patterns are similar?
```

### 2. PLACEHOLDER NODES
```
See: "1+1=2", "2+2=4", "3+3=6"

Extract common structure:
  Position 0: varies (1,2,3) → create placeholder [A]
  Position 1: "+"
  Position 2: same as position 0 → [A]  
  Position 3: "="
  Position 4: double position 0 → [2A]
  
Pattern: [A]+[A]=[2A]
```

### 3. CONTEXT SIMILARITY
```
Not: similarity(1+1, 2+2) (tokens)
Instead: similarity(context_of_1+1, context_of_2+2)

Both appear in "equation" context
Both have "number + same_number = result" structure

Connect based on STRUCTURAL similarity not token similarity!
```

### 4. COMPRESSION DISCOVERY
```
System notices:
  - "1+1=2" = 5 nodes
  - "2+2=4" = 5 nodes
  - "3+3=6" = 5 nodes
  
If can represent as: "[N]+[N]=[2N]" = 1 node → compression!

Energy reward for finding compressed representation
→ System learns to create abstract nodes
```

## Which Mechanism?

Maybe ALL of them emerge from simple rules?

Like:
- Similarity connects surface features
- Frequent similar structures create meta-patterns
- Meta-patterns get connected
- Hierarchy emerges!

But what's the MINIMAL RULE that causes this?
